import { useState, useEffect } from "react";
import "./pagesStyle.css";
import Example from "../components/Example";
import { Button, Alert, ProgressBar } from "react-bootstrap";
import { useNavigate, useLocation } from "react-router-dom";
import axios from "axios";

const AnnotationPage = (props) => {
    const navigate = useNavigate();
    const location = useLocation();
    const data = location.state.data;
    const annotatorId = location.state.annotatorId;
    const [seconds, setSeconds] = useState(new Date());
    const [currentExample, setCurrentExample] = useState(0);

    const emptyExample = {
        instruction_following_1: 3,
        depth_1: 3,
        coherence_1: 3,
        completeness_1: 3,
        factual_correctness_1: 3,
        instruction_following_2: 3,
        depth_2: 3,
        coherence_2: 3,
        completeness_2: 3,
        factual_correctness_2: 3,
        overall_preference: "",
    };

    const [exampleAnnotation, setExampleAnnotation] = useState(emptyExample);
    const [missingFields, setMissingFields] = useState([]);

    useEffect(() => {
        const interval = setInterval(() => {
            setSeconds(new Date());
        }, 1000);
        return () => clearInterval(interval);
    }, []);

    useEffect(() => {
        setExampleAnnotation(emptyExample);
    }, [currentExample]);

    const validateAnnotations = () => {
        const requiredFields = [
            "instruction_following_1",
            "depth_1",
            "coherence_1",
            "completeness_1",
            "factual_correctness_1",
            "instruction_following_2",
            "depth_2",
            "coherence_2",
            "completeness_2",
            "factual_correctness_2",
            "overall_preference"
        ];
        const missing = requiredFields.filter(field => exampleAnnotation[field] === "" || exampleAnnotation[field] === null);

        setMissingFields(missing);

        return missing.length === 0;
    };

    const handleButtonAction = () => {
        if (!validateAnnotations()) {
            return;
        }

        const endTime = new Date();
        const timeSpent = endTime - seconds;
        const updateData = {
            completed: true,
            time_spent: timeSpent,
            ...exampleAnnotation,
            annotator_id: annotatorId
        };

        axios
            .patch(`/api/annotate/example/${data[currentExample]._id}`, updateData)
            .then((response) => {
                console.log('Data saved:', response.data);
            })
            .catch((error) => {
                console.error('Error saving data:', error);
            });

        setSeconds(endTime);
        window.scrollTo(0, 0);

        if (currentExample + 1 === data.length) {
            setCurrentExample(0);
            navigate("/submission");
        } else {
            setCurrentExample(currentExample + 1);
        }
    };

    const renderAlert = () => {
        if (missingFields.length > 0) {
            return (
                <Alert variant="danger" style={{ width: "70%", marginTop: "20px", textAlign: "left" }}>
                    Please submit the following required fields before submitting: {missingFields.join(", ")}
                </Alert>
            );
        }
    };

    const renderButton = () => {
        return (
            <Button
                variant="outline-primary"
                style={{ marginLeft: "20px", fontSize: "20px" }}
                onClick={handleButtonAction}
            >
                {currentExample < data.length - 1 ? "Submit Example" : "Submit Final Example"}
            </Button>
        );
    };

    return (
        <div align="center">
            <Alert style={{ width: "70%", marginTop: "20px", textAlign: "left", fontSize: 18 }}>
                <h3> Evaluating Language Model Responses </h3>
                <br></br>
                Hello, and thank you for participating in our study! We are a group of researchers at the <a href="https://allenai.org/" target="_blank" rel="noreferrer">Allen Institute for Artificial Intelligence (Ai2)</a> building better methods for evaluating the quality of text generated by AI models like ChatGPT.
                In this task, we would like your help in <b>evaluating AI model responses to ambiguous or subjective queries</b>.
                <br></br>
                <br></br>
                Each annotation task has 2 examples:
                <ol>
                    <li> <b>Example 1</b>: You will be given a <b>query</b> and <b>two AI model responses</b>. </li>
                    <li> <b>Example 2</b>: You will be given the same <b>query</b> and <b>follow-up question-answer pairs that provide clarification about the query</b>, and <b>two AI model responses</b>. </li>
                </ol>
                <b>Steps in Annotation Task:</b><br></br><br></br>
                <ol>
                    <li>Read the query and the two responses carefully.</li><br></br>
                <li>Rate each response on a scale of 1-5 on the following criteria:
                    <br></br>
                    <ul>
                        <li><b>Instruction Following</b>: How well does the response follow all the instructions specified in the query as well as the follow-up questions?</li>
                        <li><b>Depth</b>: How precise and thorough are the details in the response?</li>
                        <li><b>Coherence</b>: How would you rate the logical flow of the response?</li>
                        <li><b>Completeness</b>: How well does the response address all aspects of the query and follow-up questions?</li>
                        <li><b>Factual Correctness</b>: How factually accurate and consistent is the information presented in the response?</li>
                    </ul>
                </li><br></br>
                <li>Next, please provide an <b>overall preference</b> for one of the two responses. If you find both responses equally good, you can select "Tie".</li>
                </ol>
                <br></br>
                Current Example: {currentExample + 1} out of {data.length}
                <ProgressBar
                    variant="primary"
                    now={((currentExample + 1) * 100.0) / data.length}
                    style={{ width: "38rem", marginTop: "20px", marginBottom: "20px" }}
                />
                <b>Important Note</b>:
                If you do not understand the query or if an error occurs in the interface, just go to the link again, enter your ID and you will be shown a different query.
                Make sure to <b>follow the instructions carefully</b> and submit all the examples!<br></br>
            </Alert>
            <Example
                query={data[currentExample].query}
                response1={data[currentExample].response1}
                response2={data[currentExample].response2}
                exampleAnnotation={exampleAnnotation}
                setExampleAnnotation={setExampleAnnotation}
            />
            {renderAlert()}
            <div className="buttons">{renderButton()}</div>
        </div>
    );
};

export default AnnotationPage;
